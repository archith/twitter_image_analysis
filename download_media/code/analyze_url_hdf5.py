# Analyze the HDF5 table generated by read_apollo dump

import pandas, os, collections, pprint, url_table
from download_instagram import save_inst_image
from download_youtube import get_youtube_vid
from download_arbitrary_web_image import save_arb_image
import util


# Download the media links one json entry at a time
def download_media_per_entry(url, url_domain, out_img_folder, out_video_folder, config=util.Config()):
    media_downloaded = False
    media_path = None
    if url_domain == url_table.INSTAGRAM_DOM:
        inst_link = url
        media_path = save_inst_image(inst_link, out_img_folder)

    elif url_domain == url_table.YOUTUBE_DOM:
        yt_link = url
        media_path = get_youtube_vid(yt_link, out_video_folder)

    # elif url_domain == url_table.TINYPIC_DOM:
    #     media_path = None
    # elif url_domain == url_table.IMGUR_DOM:
    #     media_path = None

    else:
        # For arbitrary links, we extract the largest image in the webpage
        media_path = save_arb_image(url, out_img_folder)

    if media_path:
        media_downloaded = True

    return media_downloaded, media_path


# Download the media links after all the entries have been processed into a Dataframe

def download_media(hdf5_file_name, out_img_folder, out_video_folder, config=util.Config()):

    print('Deprecated function, not supported!! Use download_media_per_entry instead.')
    assert 0
    url_df = pandas.read_hdf(hdf5_file_name)

    # Check where most of the URL stats are present

    indices = url_df.index
    columns = url_df.columns

    # Check the URL domain
    assert columns[-1] == 'url_domain'

    url_domains = url_df.loc[:, columns[-1]]

    url_domain_collection = collections.Counter(url_domains.tolist())

    pprint.pprint(url_domain_collection)

    # Find Instagram Links
    instagram_df = url_df[url_domains == url_table.INSTAGRAM_DOM]
    print('Found %d instagram links' % len(instagram_df))

    # Find youtube Links
    youtube_df = url_df[url_domains == url_table.YOUTUBE_DOM]
    print('Found %d Youtube links' % len(youtube_df))

    # Find IMGUR links
    imgur_df = url_df[url_domains == url_table.IMGUR_DOM]
    print('Found %d Imgur links' % len(imgur_df))

    # Find TINYPIC links
    tiny_df = url_df[url_domains == url_table.TINYPIC_DOM]
    print('Found %d TinyPIC links' % len(tiny_df))

    # Download from the links
    url_df['media_downloaded'] = [False] * len(url_df)
    url_df['media_file_paths'] = [''] * len(url_df)

    if (config.download_instagram):
        print('Downloading Instagram images')
        for l, idx in enumerate(instagram_df.index):
            if l % 10 == 0:
                print (l)
            inst_link = instagram_df.loc[idx, 'long_url']
            img_path = save_inst_image(inst_link, out_img_folder)
            if img_path:
                url_df.loc[idx, 'media_downloaded'] = True
                url_df.loc[idx, 'media_file_paths'] = img_path
    else:
        print('Skipping Instagram downloads')

    if config.download_yt:
        print('Downloading Youtube videos')
        for l, idx in enumerate(youtube_df.index):
            if l % 1 == 0:
                print (l)
            yt_link = youtube_df.loc[idx, 'long_url']
            yt_f_path = get_youtube_vid(yt_link, out_video_folder)
            if yt_f_path:
                url_df.loc[idx, 'media_downloaded'] = True
                url_df.loc[idx, 'media_file_paths'] = yt_f_path
    else:
        print('Skipping Youtube downloads')

    if (config.download_imgur):
        print('Downloading Imgur videos')
        for idx in imgur_df.index:
            if idx % 10 == 0:
                print (idx)
            pass  # not implemented yet
    else:
        print('Skipping Imgur downloads')

    if (config.download_tinypic):
        print('Downloading TinyPic videos')
        for idx in tiny_df.index:
            if idx % 10 == 0:
                print (idx)
            pass  # not implemented yet
    else:
        print('Skipping TinyPic downloads')

    return url_df


if __name__ == "__main__":
    json_file = '/home/archith/work/twitter_image_analytics/primaries_1/tweets.json'
    media_hdf5_file_path, hdf5_file_path, \
    _out_img_folder, _out_video_folder, _ = util.get_media_file_paths(json_file)

    # Config parameters
    config = util.Config(overwrite_media=False)
    config.set_instagram_flag(False)
    util.create_media_folders(_out_img_folder, _out_video_folder, config)

    # updated url dataframe
    url_media_df = download_media(hdf5_file_path, _out_img_folder, _out_video_folder)
    print('Saving hdf5 file with Media data in :' + media_hdf5_file_path)
    url_media_df.to_hdf(media_hdf5_file_path, 'df')
